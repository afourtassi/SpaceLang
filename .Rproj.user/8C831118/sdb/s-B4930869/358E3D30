{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Untitled\"\nauthor: \"Abdellah\"\ndate: \"2/8/2018\"\noutput: html_document\n---\n\nLibraries\n```{r}\n  library(purrr)\n  library(readr)\n  library(ggplot2)\n  library(langcog)\n  library(boot)\n#  library(lazyeval)\n  library(dplyr)\n  library(tidyr)\n  library(wordbankr)\n  library(directlabels)\n # library(scales)\n  library(stringr)\n  library(lmtest)\n  library(rwebppl)\n  library(jsonlite)\n  library(nlme)\n  library(feather)\n  library(broom)\n  library(HDInterval)\n  library(BBmisc)\n  library(data.table)\n  library(plyr)\n```\n\nThe continuous model\n\n```{r}\n\nmodel_cont <- \"\n\nvar Id = function(position, rotation){\n  return {x:position.x, y:position.y};\n};\n\n//this reflection is for front/back\nvar Ref = function(position, rotation){\n  return {x:-position.x, y:-position.y};\n};\n\n//define rotation in general\n////////////////////////////\n\nvar Rot = function(position, rotation){\n  var theta = rotation\n  var theta_rad = theta * Math.PI * 1/180\n  var rot_x= position.x*Math.cos(theta_rad)-position.y*Math.sin(theta_rad);\n  var rot_y= position.x*Math.sin(theta_rad)+position.y*Math.cos(theta_rad);\n\n  return {x:Math.round(rot_x), y:Math.round(rot_y)};\n};\n\n//define rotation for side (left-right)\nvar Rot_side = function(position, rotation){\n  return Rot(position, 90)\n}\n\n//This rotation is for non-egocentric frame of reference \nvar Rot_allo = function(position, rotation){\n  return Rot(position, rotation)\n}\n\n//define composition \nvar Comp = function (opr1, opr2){\n  return function (position, rotation){\n    opr1(opr2(position, rotation),rotation)\n  }\n}\n\n//So we are learning 4 words: both front/back, left/right, and we have 8 hypotheses\n\nvar hyp_space = [ {name:'id', operation:Id, len:1}, // front\n                  {name:'ref', operation:Ref, len:1}, // back\n                 \n                  {name:'rotSide_id', operation:Comp(Rot_side,Id), len:2}, //left ego\n                  {name:'rotSide_ref', operation:Comp(Rot_side,Ref), len:2}, // right ego\n                 \n                  {name:'rotFrame_id', operation:Comp(Rot_allo,Id), len:2}, //front allo\n                  {name:'rotFrame_ref', operation:Comp(Rot_allo,Ref), len:2}, // right allo\n                 \n                  {name:'rotFrame_Side_id', operation:Comp(Rot_allo, Comp(Rot_side, Id)), len:3}, //left allo\n                  {name:'rotFrame_Side_ref', operation:Comp(Rot_allo, Comp(Rot_side, Ref)), len:3} //right allo\n\n \n                ]\n\n\nvar hyp_names  =  map(function(hyp){\nreturn hyp.name\n}, hyp_space)\n\nvar positions = [\n                 {x:-1,y:0},\n                 {x:1, y:0},\n                 {x:0, y:1},\n                 {x:0, y:-1}\n                  ];\n\nvar utterances = ['left',\n                  'right',\n                  'front',\n                  'back'\n                 ];\n\nvar rotations = [0,\n                90,\n                180,\n                270];\n\nvar utterancePrior = Categorical({vs: utterances, ps: [1/4, 1/4, 1/4, 1/4]});\nvar positionPrior =  Categorical({vs: positions, ps: [1/4, 1/4, 1/4, 1/4]});\nvar rotationPrior =  Categorical({vs: rotations, ps: [1/4, 1/4, 1/4, 1/4]});\n\n//Every word (e.g., 'left' , 'right') has a probability distribution over all hypotheses \n// have a different prioir for each word and each hypothesis\n//let's first try different infernce for each word \n\nvar wordPrior = function(word) { \n    var hypProbs = T.toScalars(dirichlet(ones([hyp_names.length, 1])))\n    return Categorical({vs: hyp_names, ps: hypProbs})\n}\n\n\n//Egocentric Meaning: returns true if the word maps to the correct position  \nvar meaning = function(utt, position, rotation) {\n    var rot_position = Rot(position, -rotation)\n    return utt=='front'? (rot_position.x==0 && rot_position.y==1):\n    utt=='back'? (rot_position.x==0 && rot_position.y==-1):\n    utt=='left'? (rot_position.x==-1 && rot_position.y==0):\n    utt=='right'? (rot_position.x==1 && rot_position.y==0):\n    true\n};\n\n//Speaker: has in mind a position, and utter the corresponding word based on rotation \nvar S = cache(function(position, rotation) {\n  return Infer({method:'enumerate'}, function(){\n    var utt = sample(utterancePrior);\n    condition(meaning(utt, position, rotation));\n    return utt;\n  });\n});\n\n\nvar data = input  \n\nvar alpha=data[0].alpha\nvar beta=data[0].beta\nvar myWord = data[0].word \n\n//Listner: hears a word, sees a rotation and try to associate it with a meaning from the space\n\n//Do a conditional (if the word is the same )\nvar L1 = function(trueRotation, wordDist) {\n  return Infer({method:'enumerate'}, function(){\n    var rep = uniformDraw(hyp_space); //sample a representation\n    var rep_opr = rep.operation\n    var response =  rep_opr({x:0,y:1}, trueRotation) \n  \n    factor(alpha*wordDist.score(rep.name)-beta*rep.len)\n    //factor(-beta*rep.len)\n    //factor(alpha*wordDist.score(rep.name)+beta*rep.len)\n    return [response.x, response.y]\n  });\n};\n\n\n\nvar wordPost = function(word, data) {\n  return Infer({method : 'MCMC', samples : 10000, burn: 2000}, function() {\n    var wordDist = wordPrior(word);\n    mapData({data: data}, function(datum){\n      if (datum.word == word) {observe(L1(datum.rot, wordDist), [datum.x, datum.y])}\n    });\n    return wordDist;\n  });\n};\n\n\n//model()\nvar learnData = function(size) {\n  \n  var step = function(data) {\n    if(data.length == size) return data;\n    \n    var position = sample(positionPrior)\n    var rotation = sample(rotationPrior)\n    var utt = sample(S(position, rotation))\n    \n    var newDatum = {x: position.x, y: position.y, rot: rotation, word: utt}\n  \n  return  step(data.concat(newDatum));\n    \n  }\n  \n    step([]);\n}\n\n\nvar model =  function () {\n  var data = learnData(20)\n  var hyps = map(function(w){ \n      return wordPost(w, data).MAP().val;\n    }, utterances)\n  return _.object(utterances, hyps);\n};\n\n//var data = learnData(10)\n\nwordPost(myWord, data).MAP().val\n\n\"\n```\n\n```{r}\nx=c(1, 1,  1, -1)\ny=c(0, 0, 0, 0)\nrot=c(0, 0, 0, 180)\nword='right'\n\nN = 4\n\ninput = data.frame(alpha = 2, beta = 2, x=rep(x,3), y=rep(y,3), rot=rep(rot,3), word = rep(rep(word, 4), 3)) %>% head(N)\n      \npost_all <- data.frame()\n\nfor (i in seq(1,2)) {\nposterior <- webppl(program_file = \"models/model_2words.wppl\", \n                    data = input,\n                    data_var = \"input\")\nposterior <- posterior %>%\n  mutate(mcmc = i)\n\npost_all <- bind_rows(post_all, posterior)\n\n\n}\n\ndata_sum <- post_all %>%\n  group_by(support) %>%\n  summarise(mean = mean(prob),\n            sd = sd(prob))\n\n\nggplot(post_all, aes(x=N, y=mean, col=hypothese))+\n  \n  geom_pointrange(aes(ymin = lower, ymax = upper), \n                  position = position_dodge(width = .1)) + \n  geom_line()+\n  \n  facet_grid(hypothese  ~ beta) +\n  theme(aspect.ratio = 0.7, axis.text.x = element_text(angle = 90, hjust = 1))\n\n```\n\n\n```{r}\n\nmcmc_values = data.frame()\n\nnames =  c(posterior[[1,3]][1], posterior[[1,3]][2], posterior[[1,3]][3], posterior[[1,3]][4], \n             posterior[[1,3]][5], posterior[[1,3]][6], posterior[[1,3]][7], posterior[[1,3]][8])\n\nfor (i in 1:nrow(posterior)) {\n  \n    line = data.frame(posterior[[i,2]][1], posterior[[i,2]][2], posterior[[i,2]][3], posterior[[i,2]][4], \n             posterior[[i,2]][5], posterior[[i,2]][6], posterior[[i,2]][7], posterior[[i,2]][8])\n    \n    mcmc_values = bind_rows(mcmc_values, line)\n\n}\n\ncolnames(mcmc_values) <- names\n\nmeans <- data.frame(sapply(mcmc_values, mean)) %>%\n  rename(mean = sapply.mcmc_values..mean.)\n\nmeans <- cbind(rownames(means), means)\nrownames(means) <- NULL\ncolnames(means) <- c('hypothese', 'mean')\n\n\nsds <- data.frame(sapply(mcmc_values, sd)) %>%\n  rename(sd = sapply.mcmc_values..sd.)\n\nsds <- cbind(rownames(sds), sds)\nrownames(sds) <- NULL\ncolnames(sds) <- c('hypothese', 'sd')\n\nall <- means %>%\n  left_join(sds)\n\nsapply(mcmc_values, sd) \nmy_sum <- mcmc_values %>%\n  sapply(mean)\n  group_by(key) %>%\n  summarize(mean = mean(value), \n            sd = sd(value))\n\n```\n\n\n\nInput data and run continuous model\n\n\nHere model with only two words \n\n```{r}\n\nalpha_val = c(2)\nbeta_val = c(1, 1.25, 1.5, 1.75, 2, 4, 8)\nN_val = seq(1, 12)\n\ndata_right_exp_cont = data.frame()\n\nx=c(1, 1,  1, -1)\ny=c(0, 0, 0, 0)\nrot=c(0, 0, 0, 180)\n\n\nword='right'\n\nfor (alpha in alpha_val){\n  for(beta in beta_val){\n    for(N in N_val){\n      \n      input = data.frame(alpha = alpha, beta = beta, x=rep(x,3), y=rep(y,3), rot=rep(rot,3), word = rep(rep(word, 4), 3)) %>% head(N)\n      \n      post_all <- data.frame()\n      for (i in seq(1,10)) {\n      posterior <- NULL\n      while (typeof(posterior)==\"NULL\") {\n       posterior <- webppl(program_file = \"models/model_2words.wppl\", \n                    data = input,\n                    data_var = \"input\") \n      }\n      \n      posterior <- posterior %>%\n        mutate(mcmc = i)\n      \n      post_all <- bind_rows(post_all, posterior)\n      \n      }\n      \n    \n      learning <- post_all %>%\n        mutate(word = word, \n               alpha= alpha,\n               beta = beta,\n               N=N)\n      \n      data_right_exp_cont = bind_rows(data_right_exp_cont, learning)\n    }\n    \n  }\n  \n}\n\n\n```\n\n\nhere plot\n\n```{r}\n\n#feather::write_feather(data_front_exp_cont, \"moreEgoInput_front.feather\")\n\ndata_explore  <- data_right_exp_cont  #%>%\n  #filter(word == \"right\")#,\n         #beta < alpha |  beta == alpha) \n  \n  \n\ndata_explore$support = factor(data_explore$support, levels = c(\"left_ego\", \"right_ego\", \"left_allo\", \"right_allo\"))\n\nright_alpha <- ggplot(data_explore, aes(x=N, y=prob))+\n  \n  geom_point()+\n  geom_smooth()+\n  \n  facet_grid(support  ~ beta) +\n  theme(aspect.ratio = 0.7, axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n```\n\n\n-One way to represent the data: I can show only the average over the 4 positions \n```{r}\n\nalpha_val = c(2)\nbeta_val = c(2)\nN_val = seq(1, 12)\n\ndata_right_exp_cont = data.frame()\n\nx=c(1,  0, -1, 0)\ny=c(0, 1, 0, -1)\nrot=c(0, 90, 180, 270)\n\nx=c(1,  1, 1, 0)\ny=c(0, 0, 0, -1)\nrot=c(0, 0, 0, 270)\n\nx=c(1, 1,  1, -1)\ny=c(0, 0, 0, 0)\nrot=c(0, 0, 0, 180)\n\n\nword='right'\n\nfor (alpha in alpha_val){\n  for(beta in beta_val){\n    for(N in N_val){\n      \n      input = data.frame(alpha = alpha, beta = beta, x=rep(x,3), y=rep(y,3), rot=rep(rot,3), word = rep(rep(word, 4), 3)) %>% head(N)\n      \n      post_all <- data.frame()\n      for (i in seq(1,10)) {\n      posterior <- NULL\n      while (typeof(posterior)==\"NULL\") {\n       posterior <- webppl(model_cont, \n                    data = input,\n                    data_var = \"input\") \n      }\n      \n      posterior <- posterior %>%\n        mutate(mcmc = i)\n      \n      post_all <- bind_rows(post_all, posterior)\n      \n      }\n      \n    \n      learning <- post_all %>%\n        mutate(word = word, \n               alpha= alpha,\n               beta = beta,\n               N=N)\n      \n      data_right_exp_cont = bind_rows(data_right_exp_cont, learning)\n    }\n    \n  }\n  \n}\n\n\nalpha_val = c(2)\nbeta_val = c(6, 8)\nN_val = seq(1, 12)\n\ndata_front_exp_cont = data.frame()\n\nx=c(0, -1, 0, 1)\ny=c(1, 0, -1, 0)\nrot=c(0, 90, 180, 270)\n\nx=c(0, 0, 0, 1)\ny=c(1, 1, 1, 0)\nrot=c(0, 0, 0, 270)\n\nword='front'\n\nfor (alpha in alpha_val){\n  for(beta in beta_val){\n    for(N in N_val){\n      \n      input = data.frame(alpha = alpha, beta = beta, x=rep(x,3), y=rep(y,3), rot=rep(rot,3), word = rep(rep(word, 4), 3)) %>% head(N)\n  \n      \n      post_all <- data.frame()\n      for (i in seq(1,10)) {\n      posterior <- NULL\n      while (typeof(posterior)==\"NULL\") {\n       posterior <- webppl(model_cont, \n                    data = input,\n                    data_var = \"input\") \n      }\n      \n      posterior <- posterior %>%\n        mutate(mcmc = i)\n      \n      post_all <- bind_rows(post_all, posterior)\n      \n      }\n      \n    \n      learning <- post_all %>%\n        mutate(word = word, \n               alpha= alpha,\n               beta = beta,\n               N=N)\n      \n      data_front_exp_cont = bind_rows(data_front_exp_cont, learning)\n    }\n    \n  }\n  \n}\n\n```\n\n\n\n```{r}\n\nfeather::write_feather(data_front_exp_cont, \"moreEgoInput_front.feather\")\n\ndata_explore  <- data_front_exp_cont  #%>%\n  #filter(word == \"right\")#,\n         #beta < alpha |  beta == alpha) \n  \n  \ndata_explore$support <- mapvalues(data_explore$support, \n                                     from = c(\"id\", \"ref\",  \n                                              \"rotSide_id\",\"rotSide_ref\",\n                                              \"rotFrame_id\",\"rotFrame_ref\",\n                                              \"rotFrame_Side_id\",\"rotFrame_Side_ref\"),\n                                     to = c(\"front_ego\", \"back_ego\", \n                                            \"left_ego\", \"right_ego\", \n                                            \"front_allo\", \"back_allo\", \n                                            \"left_allo\", \"right_allo\"))\n\ndata_explore$support = factor(data_explore$support, levels = c(\"front_ego\", \"back_ego\", \"left_ego\", \"right_ego\", \"front_allo\", \"back_allo\", \"left_allo\", \"right_allo\"))\n\nright_alpha <- ggplot(data_explore, aes(x=N, y=prob))+\n  \n  geom_point()+\n  geom_smooth()+\n  \n  facet_grid(support  ~ beta) +\n  theme(aspect.ratio = 0.7, axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n```\n\n\nIdeas for how to present the data\n\n-Posterior probability as a function of the data \n->I should see that front-back converge first (with the posterior probability ->1)\n->Left-right converge second (posteririor ->)\n->Ego centric probability first, Allo-centric second\n-Left-right, and front-back converge at the same time \n\nImplement the continuous version \n\nexplore the behavior of the model under:\n-Different values of alpha, and beta (especially beta)\n-Different distribution of the input (the convergence is delayed if the initial input is dominated by ago-centric representation).\n\n\nInput to the model should be \n-Alpha, beta\n-Number of training data\n-orientation 0 vs [0,180] vs. [0, 90, 180, 270]\n\nRotation 0\n-The siplest case, No ambiguity, words are associated with fixed spatial positions\n\nRotation 0, 180\n-left-right data is orthogonal to front-back, ambiguity is within the symmetry only \n\nRotation 0, 90, 180, 270\n-Maximal ambiguity, all 4 words can be confused\n\nIn terms of the input\nThe input can also make the problem more or less easy\n-If we start with non-overlapping input? \n\n\nPreliminary results\n-We have interesting interaction between the parameter that penalizes length and the number of data needed to converge (altough the coarse granularity of the space of probabilities does not allow us to see if there is a little advantage to accurate (longest) representation).\ninput:\n-Number of datapoints\n-The values of alpha/beta\n\n\n\n\nEexperimenting here\n\n```{r}\nalpha_val = c(2)\nbeta_val = c(1, 1.75)\nN_val = seq(1,8)\n\n#Right/Left\n#############\n\ndata_right_exp = data.frame()\n\nx=c(1, 0, -1, 0)\ny=c(0, 1, 0, -1)\nrot=c(0, 90, 180, 270)\nword=c('right', 'right', 'right', 'right')\n\nfor (alpha in alpha_val){\n  for(beta in beta_val){\n    for(N in N_val){\n      \n      input = data.frame(alpha = alpha, beta = beta, x= rep(x,3), y = rep(y,3), rot = rep(rot, 3), word = rep(word, 3)) %>% head(N)\n      \n      posterior <- NULL\n      while (typeof(posterior)==\"NULL\") {\n       posterior <- webppl(model, \n                    data = input,\n                    data_var = \"input\")\n    }\n      \n      learning_norm <- posterior %>%\n        rbindlist(idcol = TRUE) %>%\n        gather(hypothese, prob, id:rotFrame_Side_ref) %>%\n        dplyr::group_by(.id) %>%\n        dplyr::mutate(prob = prob/sum(prob)) %>%\n        dplyr::rename(word = .id) %>%\n        dplyr::mutate(alpha = alpha,\n                      beta = beta,\n                      N = N)\n      \n      data_right_exp = bind_rows(data_right_exp, learning_norm)\n      \n    }\n  }\n}\n\n\ndata_front_exp = data.frame()\n\n#Front/back\n#############\n\nx=c(0, -1, 0, 1)\ny=c(1, 0, -1, 0)\nrot=c(0, 90, 180, 270)\nword=c('front', 'front', 'front', 'front')\n\n\nfor (alpha in alpha_val){\n  for(beta in beta_val){\n    for(N in N_val){\n      \n      input = data.frame(alpha = alpha, beta = beta, x= rep(x,3), y = rep(y,3), rot = rep(rot, 3), word = rep(word, 3)) %>% head(N)\n      \n      posterior <- NULL\n      while (typeof(posterior)==\"NULL\") {\n       posterior <- webppl(model, \n                    data = input,\n                    data_var = \"input\")\n    }\n      \n      learning_norm <- posterior %>%\n        rbindlist(idcol = TRUE) %>%\n        gather(hypothese, prob, id:rotFrame_Side_ref) %>%\n        dplyr::group_by(.id) %>%\n        dplyr::mutate(prob = prob/sum(prob)) %>%\n        dplyr::rename(word = .id) %>%\n        dplyr::mutate(alpha = alpha,\n                      beta = beta,\n                      N = N)\n      \n      data_front_exp = bind_rows(data_front_exp, learning_norm)\n      \n    }\n  }\n}\n\n\n\n```\n\n\n\n```{r}\nalpha_val = c(1, 2, 4, 8, 16)\nbeta_val = c(1, 2, 4, 8, 16)\nN_val = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)\n\n#Right/Left\n#############\n\ndata_right = data.frame()\n\nx=c(1, 0, -1, 0)\ny=c(0, 1, 0, -1)\nrot=c(0, 90, 180, 270)\nword=c('right', 'right', 'right', 'right')\n\nfor (alpha in alpha_val){\n  for(beta in beta_val){\n    for(N in N_val){\n      \n      input = data.frame(alpha = alpha, beta = beta, x= rep(x,3), y = rep(y,3), rot = rep(rot, 3), word = rep(word, 3)) %>% head(N)\n      \n      posterior <- NULL\n      while (typeof(posterior)==\"NULL\") {\n       posterior <- webppl(model, \n                    data = input,\n                    data_var = \"input\")\n    }\n      \n      learning_norm <- posterior %>%\n        rbindlist(idcol = TRUE) %>%\n        gather(hypothese, prob, id:rotFrame_Side_ref) %>%\n        dplyr::group_by(.id) %>%\n        dplyr::mutate(prob = prob/sum(prob)) %>%\n        dplyr::rename(word = .id) %>%\n        dplyr::mutate(alpha = alpha,\n                      beta = beta,\n                      N = N)\n      \n      data_right = bind_rows(data_right, learning_norm)\n      \n    }\n  }\n}\n\ndata_front = data.frame()\n\n#Front/back\n#############\n\nx=c(0, -1, 0, 1)\ny=c(1, 0, -1, 0)\nrot=c(0, 90, 180, 270)\nword=c('front', 'front', 'front', 'front')\n\n\nfor (alpha in alpha_val){\n  for(beta in beta_val){\n    for(N in N_val){\n      \n      input = data.frame(alpha = alpha, beta = beta, x= rep(x,3), y = rep(y,3), rot = rep(rot, 3), word = rep(word, 3)) %>% head(N)\n      \n      posterior <- NULL\n      while (typeof(posterior)==\"NULL\") {\n       posterior <- webppl(model, \n                    data = input,\n                    data_var = \"input\")\n    }\n      \n      learning_norm <- posterior %>%\n        rbindlist(idcol = TRUE) %>%\n        gather(hypothese, prob, id:rotFrame_Side_ref) %>%\n        dplyr::group_by(.id) %>%\n        dplyr::mutate(prob = prob/sum(prob)) %>%\n        dplyr::rename(word = .id) %>%\n        dplyr::mutate(alpha = alpha,\n                      beta = beta,\n                      N = N)\n      \n      data_front = bind_rows(data_front, learning_norm)\n      \n    }\n  }\n}\n\n```\n\n\n```{r}\n\ndata_explore <- data_right_exp %>%\n  filter(word == \"right\")#,\n         #beta < alpha |  beta == alpha) \n  \n  \ndata_explore$hypothese <- mapvalues(data_explore$hypothese, \n                                     from = c(\"id\", \"ref\",  \n                                              \"rotSide_id\",\"rotSide_ref\",\n                                              \"rotFrame_id\",\"rotFrame_ref\",\n                                              \"rotFrame_Side_id\",\"rotFrame_Side_ref\"),\n                                     to = c(\"front_ego\", \"back_ego\", \n                                            \"left_ego\", \"right_ego\", \n                                            \"front_allo\", \"back_allo\", \n                                            \"left_allo\", \"right_allo\"))\n\ndata_explore$hypothese = factor(data_explore$hypothese, levels = c(\"front_ego\", \"back_ego\", \"left_ego\", \"right_ego\", \"front_allo\", \"back_allo\", \"left_allo\", \"right_allo\"))\n\nright_alpha2 <- ggplot(subset(data_explore, alpha==2), aes(x=hypothese, y=prob))+\n  geom_col()+\n  facet_grid(N  ~ beta) +\n  theme(aspect.ratio = 0.7, axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n```\n\n```{r}\n\ndata_explore <- data_front_exp %>%\n  filter(word == \"front\")#,\n         #beta < alpha |  beta == alpha) \n  \n  \ndata_explore$hypothese <- mapvalues(data_explore$hypothese, \n                                     from = c(\"id\", \"ref\",  \n                                              \"rotSide_id\",\"rotSide_ref\",\n                                              \"rotFrame_id\",\"rotFrame_ref\",\n                                              \"rotFrame_Side_id\",\"rotFrame_Side_ref\"),\n                                     to = c(\"front_ego\", \"back_ego\", \n                                            \"left_ego\", \"right_ego\", \n                                            \"front_allo\", \"back_allo\", \n                                            \"left_allo\", \"right_allo\"))\n\ndata_explore$hypothese = factor(data_explore$hypothese, levels = c(\"front_ego\", \"back_ego\", \"left_ego\", \"right_ego\", \"front_allo\", \"back_allo\", \"left_allo\", \"right_allo\"))\n\nfront_alpha2 <- ggplot(subset(data_explore, alpha==2), aes(x=hypothese, y=prob))+\n  geom_col()+\n  facet_grid(N  ~ beta) +\n  theme(aspect.ratio = 0.7, axis.text.x = element_text(angle = 90, hjust = 1))\n\nfront_alpha4 <- ggplot(subset(data_explore, alpha==4), aes(x=hypothese, y=prob))+\n  geom_col()+\n  facet_grid(N  ~ beta) +\n  theme(aspect.ratio = 0.7, axis.text.x = element_text(angle = 90, hjust = 1))\n\nfront_alpha8 <- ggplot(subset(data_explore, alpha==8), aes(x=hypothese, y=prob))+\n  geom_col()+\n  facet_grid(N  ~ beta) +\n  theme(aspect.ratio = 0.7, axis.text.x = element_text(angle = 90, hjust = 1))\n\nfront_alpha16 <- ggplot(subset(data_explore, alpha==16), aes(x=hypothese, y=prob))+\n  geom_col()+\n  facet_grid(N  ~ beta) +\n  theme(aspect.ratio = 0.7, axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n```\n\n\n\nPreliminary results\n-We learn the egocentric representation before the allocentric representation (requires a relatively high value of beta)\n-We learn the front-back representation before the left-right representation (because left-right is derived from front and back)\n-The developmental data allows to combine the effect of input abd t\n\n \nThe data\n```{r}\n\n#Build training data, depending on what phenomenon we would like to investigate\n\n#Input for \"right\"\nx=c(1, 0, -1, 0)\ny=c(0, 1, 0, -1)\nrot=c(0, 90, 180, 270)\nword=c('right', 'right', 'right', 'right')\n\nright <- data.frame(x, y, rot, word)\n\n#right <- data.frame(rep(x, 2), rep(y, 2), rep(rot, 2), rep(word, 2))\n\n\n#Input to \"front\"\nx=c(0, -1, 0, 1)\ny=c(1, 0, -1, 0)\nrot=c(0, 90, 180, 270)\nword=c('front', 'front', 'front', 'front')\n\nfront <- data.frame(rep(x, 2), rep(y, 2), rep(rot, 2), rep(word, 2))\n  \nx = c(1, 1,-1,-1,1, -1, -1, -1,-1, -1, 1, 1,-1,-1,1, -1, -1, -1,-1, -1)\ny = c(0, 0,0,0,0, 0, 0,0, 0,0, 0, 0,0,0,0, 0, 0,0, 0,0)\nrot = c(0, 180, 180, 0, 0,0,0,0,0,0,0, 0,0,0,0, 0, 0,0, 0,0)\nword= c('right', 'left', 'right', 'left', 'right', 'left', 'left','left','left','left',\n        'left','left','left','left','left','left','left','left','left','left')\n\ntraining = data.frame(x, y, rot, word)\n\ntraining_1 <- head(training, 1)\ntraining_2 <- head(training, 2)\ntraining_3 <- head(training, 3)\ntraining_4 <- head(training, 4)\ntraining_5 <- head(training, 5)\ntraining_6 <- head(training, 6)\ntraining_7 <- head(training, 7)\ntraining_8 <- head(training, 8)\ntraining_9 <- head(training, 9)\ntraining_10 <- head(training, 10)\ntraining_11 <- head(training, 11)\ntraining_12 <- head(training, 12)\ntraining_13 <- head(training, 13)\ntraining_14 <- head(training, 14)\ntraining_15 <- head(training, 15)\ntraining_16 <- head(training, 16)\ntraining_17 <- head(training, 17)\ntraining_18 <- head(training, 18)\ntraining_19 <- head(training, 19)\ntraining_20 <- head(training, 20)\n\n```\n\n\n```{r}\n\nposterior <- webppl(model)\nstart = Sys.time()\nposterior_1 <- webppl(model, \n                    data = training_1,\n                    data_var = \"training\")\nprint(Sys.time() - start)\n\nstart = Sys.time()\nposterior_2 <- webppl(model, \n                    data = training_2,\n                    data_var = \"training\")\nprint(Sys.time() - start)\n\nstart = Sys.time()\nposterior_3 <- webppl(model, \n                    data = training_3,\n                    data_var = \"training\")\nprint(Sys.time() - start)\n\nstart = Sys.time()\nposterior_4 <- webppl(model, \n                    data = training_4,\n                    data_var = \"training\")\nprint(Sys.time() - start)\n\nstart = Sys.time()\nposterior_5 <- webppl(model, \n                    data = training_5,\n                    data_var = \"training\")\nprint(Sys.time() - start)\n\nstart = Sys.time()\n posterior_6 <- webppl(model, \n                     data = training_6,\n                     data_var = \"training\")\n \nprint(Sys.time() - start)\n\nstart = Sys.time()\n posterior_7 <- webppl(model, \n                     data = training_7,\n                     data_var = \"training\")\n \nprint(Sys.time() - start)\n\nstart = Sys.time()\n posterior_8 <- webppl(model, \n                     data = training_8,\n                     data_var = \"training\")\n \nprint(Sys.time() - start)\n \nstart = Sys.time()\n posterior_9 <- webppl(model, \n                     data = training_9,\n                     data_var = \"training\")\n \nprint(Sys.time() - start)\n\nstart = Sys.time()\n posterior_10 <- webppl(model, \n                     data = training_10,\n                     data_var = \"training\")\n \nprint(Sys.time() - start)\n\nstart = Sys.time()\nposterior_11 <- webppl(model, \n                    data = training_11,\n                    data_var = \"training\")\nprint(Sys.time() - start)\n\nstart = Sys.time()\nposterior_12 <- webppl(model, \n                    data = training_12,\n                    data_var = \"training\")\nprint(Sys.time() - start)\n\nstart = Sys.time()\nposterior_13 <- webppl(model, \n                    data = training_13,\n                    data_var = \"training\")\nprint(Sys.time() - start)\n\nstart = Sys.time()\nposterior_14 <- webppl(model, \n                    data = training_14,\n                    data_var = \"training\")\nprint(Sys.time() - start)\n\nstart = Sys.time()\nposterior_15 <- webppl(model, \n                    data = training_15,\n                    data_var = \"training\")\nprint(Sys.time() - start)\n\nstart = Sys.time()\n posterior_16 <- webppl(model, \n                     data = training_16,\n                     data_var = \"training\")\n \nprint(Sys.time() - start)\n\nstart = Sys.time()\n posterior_17 <- webppl(model, \n                     data = training_17,\n                     data_var = \"training\")\n \nprint(Sys.time() - start)\n\nstart = Sys.time()\n posterior_18 <- webppl(model, \n                     data = training_18,\n                     data_var = \"training\")\n \nprint(Sys.time() - start)\n \nstart = Sys.time()\n posterior_19 <- webppl(model, \n                     data = training_19,\n                     data_var = \"training\")\n \nprint(Sys.time() - start)\n\nstart = Sys.time()\n posterior_20 <- webppl(model, \n                     data = training_20,\n                     data_var = \"training\")\n \nprint(Sys.time() - start)\n\niter1 <- posterior_1[which.max(posterior_1$prob),]\niter2 <- posterior_2[which.max(posterior_2$prob),]\niter3 <- posterior_3[which.max(posterior_3$prob),]\niter4 <- posterior_4[which.max(posterior_4$prob),]\niter5 <- posterior_5[which.max(posterior_5$prob),]\niter6 <- posterior_6[which.max(posterior_6$prob),]\n\n\n```\n\n\nGet data \n```{r}\n#split by word\n#split by hypothese\n#Normalize by sclae \n\n#iterations = list(posterior_1, posterior_2, posterior_3, posterior_4, posterior_5, posterior_6)\niterations = list(iter1, iter2, iter3, iter4, iter5, iter6)\nlearning = data.frame()\n\n\nfor (i in 1:length(iterations)) {\n\nleft <- iterations[[i]] %>%\n  dplyr::select(starts_with(\"left\")) %>%\n  gather(hypothese, prob) %>%\n  dplyr::mutate(hypothese = gsub(\"left.\", \"\", hypothese )) %>%\n  dplyr::mutate(word  = \"left\")\n\nright <- iterations[[i]] %>%\n  dplyr::select(starts_with(\"right\")) %>%\n  gather(hypothese, prob) %>%\n  dplyr::mutate(hypothese = gsub(\"right.\", \"\", hypothese )) %>%\n  dplyr::mutate(word  = \"right\")\n\nlexicon = bind_rows(left, right) %>%\n  mutate(iteration = i)\n  \n  learning =  bind_rows(learning, lexicon)\n  \n}\n\nlearning_norm <- learning %>%\n  dplyr::group_by(iteration, word) %>%\n  dplyr::mutate(prob = prob/sum(prob))\n\n#Reorder the hypotheses for the plot\n\n\nlearning_norm$hypothese <- mapvalues(learning_norm$hypothese, from = c(\"rotSide_id\",\"rotSide_ref\",\"rotFrame_Side_id\",\"rotFrame_Side_ref\"), to = c(\"left_ego\", \"right_ego\", \"left_allo\", \"right_allo\"))\n\nlearning_norm$hypothese = factor(learning_norm$hypothese, levels = c(\"left_ego\", \"right_ego\", \"left_allo\", \"right_allo\"))\n\nggplot(learning_norm, aes(x=hypothese, y=prob))+\n  geom_col()+\n  facet_grid(word ~ iteration) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n```\n\n\n",
    "created" : 1522950935375.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2889206589",
    "id" : "358E3D30",
    "lastKnownWriteTime" : 1522953123,
    "last_content_update" : 1522955856146,
    "path" : "~/Documents/Experiments/SpaceLang/space_main.Rmd",
    "project_path" : "space_main.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}